{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten, Activation, MaxPooling1D, Dropout\n",
    "from tensorflow.keras.utils import plot_model,to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for each data point in dataset (stores id, diagnosis, and path to audio file)\n",
    "class Diagnosis():\n",
    "    def __init__(self, id, diagnosis, path):\n",
    "        self.id = id\n",
    "        self.diagnosis = diagnosis\n",
    "        self.path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns list of paths to all audio files and path to audio files directory\n",
    "def get_audio_files():\n",
    "    audio_path = 'D:/Summer_Intern/archive/Respiratory_Sound_Database/Respiratory_Sound_Database/audio_and_txt_files/'\n",
    "    files = [f for f in listdir(audio_path) if isfile(join(audio_path, f))] # Gets all files in the audio_and_txt_files folder\n",
    "    audio_files = [f for f in files if f.endswith('.wav')] # Gets all audio files (.wav)\n",
    "    audio_files = sorted(audio_files)\n",
    "    return audio_files, audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processes all diagnoses and creates Diagnosis objects for each data point in dataset\n",
    "def diagnosis_data():\n",
    "    diagnosis = pd.read_csv('D:/Summer_Intern/archive/Respiratory_Sound_Database/Respiratory_Sound_Database/patient_diagnosis.csv')\n",
    "    audio_files, audio_path = get_audio_files()\n",
    "    diag_dict = { 101: \"URTI\" } # dictionary w/ id and diagnosis\n",
    "    diag_list = []\n",
    "    \n",
    "    for index, row in diagnosis.iterrows():\n",
    "        diag_dict[row[0]] = row[1]\n",
    "    \n",
    "    i = 0\n",
    "    for f in audio_files:\n",
    "        diag_list.append(Diagnosis(i, diag_dict[int(f[:3])], audio_path+f))\n",
    "        i += 1\n",
    "    return diag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_features(filename):\n",
    "    sound, sample_rate = librosa.load(filename)\n",
    "    stft = np.abs(librosa.stft(sound)) # short-time Fourier transform (STFT) spectrogram\n",
    "    \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=sound, sr=sample_rate, n_mfcc=40), axis=1) # Mel-frequency cepstral coefficients\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate), axis=1) # Chromagram from STFT spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=sound, sr=sample_rate), axis=1) # Mel-scaled spectrogram\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate), axis=1) # Spectral contrast\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(sound), sr=sample_rate), axis=1) # Tonal centroid features\n",
    "    \n",
    "    features = np.concatenate((mfccs, chroma, mel, contrast, tonnetz))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_points():\n",
    "    labels = []\n",
    "    images = []\n",
    "    \n",
    "    encoding = { \"COPD\":0, \"Healthy\":1, \"URTI\":2, \"Bronchiectasis\":3, \"Pneumonia\":4, \"Bronchiolitis\":5, \"Asthma\":6, \"LRTI\":7 }\n",
    "    \n",
    "    i = 0\n",
    "    for f in diagnosis_data():\n",
    "        print(i)\n",
    "        labels.append(encoding[f.diagnosis])\n",
    "        images.append(audio_features(f.path))\n",
    "        i += 1\n",
    "    return np.array(labels), np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(labels, images):\n",
    "    # Remove Asthma and LRTI because lack of data for those, will only hurt the model\n",
    "    images = np.delete(images, np.where((labels == 7) | (labels == 6))[0], axis=0) \n",
    "    labels = np.delete(labels, np.where((labels == 7) | (labels == 6))[0], axis=0)      \n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=10)\n",
    "\n",
    "    # Hot one encode the labels\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    \n",
    "    # Format new data\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], 6))\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    y_test = np.reshape(y_test, (y_test.shape[0], 6))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_train.shape[1],  1))\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving and processing data...\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yiyon\\AppData\\Local\\Temp\\ipykernel_1784\\3618766496.py:9: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  diag_dict[row[0]] = row[1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieving and processing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m labels, images \u001b[38;5;241m=\u001b[39m \u001b[43mdata_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m preprocessing(labels, images)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished data retrieval and preprocessing!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mdata_points\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m     10\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(encoding[f\u001b[38;5;241m.\u001b[39mdiagnosis])\n\u001b[1;32m---> 11\u001b[0m     images\u001b[38;5;241m.\u001b[39mappend(\u001b[43maudio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     12\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(labels), np\u001b[38;5;241m.\u001b[39marray(images)\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36maudio_features\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      5\u001b[0m mfccs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39msound, sr\u001b[38;5;241m=\u001b[39msample_rate, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Mel-frequency cepstral coefficients\u001b[39;00m\n\u001b[0;32m      6\u001b[0m chroma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mchroma_stft(S\u001b[38;5;241m=\u001b[39mstft, sr\u001b[38;5;241m=\u001b[39msample_rate), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Chromagram from STFT spectrogram\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m mel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Mel-scaled spectrogram\u001b[39;00m\n\u001b[0;32m      8\u001b[0m contrast \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mspectral_contrast(S\u001b[38;5;241m=\u001b[39mstft, sr\u001b[38;5;241m=\u001b[39msample_rate), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Spectral contrast\u001b[39;00m\n\u001b[0;32m      9\u001b[0m tonnetz \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mtonnetz(y\u001b[38;5;241m=\u001b[39mlibrosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mharmonic(sound), sr\u001b[38;5;241m=\u001b[39msample_rate), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Tonal centroid features\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "print(\"Retrieving and processing data...\")\n",
    "labels, images = data_points()\n",
    "X_train, X_test, y_train, y_test = preprocessing(labels, images)\n",
    "print(\"Finished data retrieval and preprocessing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(64, kernel_size=5, activation='relu', input_shape=(193, 1)))\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(2)) \n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))   \n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=70, batch_size=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=60, verbose=0)\n",
    "print('Accuracy: {0:.0%}'.format(score[1]/1))\n",
    "print(\"Loss: %.4f\\n\" % score[0])\n",
    "\n",
    "# Plot accuracy and loss graphs\n",
    "plt.figure(figsize = (15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label = 'training acc')\n",
    "plt.plot(history.history['val_accuracy'], label = 'validation acc')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], label = 'validation loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_index = [\"COPD\", \"Healthy\", \"URTI\", \"Bronchiectasis\", \"Pneumonia\", \"Bronchiolitis\"]\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "classpreds = np.argmax(preds, axis=1) # predicted classes \n",
    "y_testclass = np.argmax(y_test, axis=1) # true classes\n",
    "\n",
    "cm = confusion_matrix(y_testclass, classpreds)\n",
    "print(classification_report(y_testclass, classpreds, target_names=matrix_index))\n",
    "\n",
    "# Get percentage value for each element of the matrix\n",
    "cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "cm_perc = cm / cm_sum.astype(float) * 100\n",
    "annot = np.empty_like(cm).astype(str)\n",
    "nrows, ncols = cm.shape\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        c = cm[i, j]\n",
    "        p = cm_perc[i, j]\n",
    "        if i == j:\n",
    "            s = cm_sum[i]\n",
    "            annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "        elif c == 0:\n",
    "            annot[i, j] = ''\n",
    "        else:\n",
    "            annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "\n",
    "\n",
    "# Display confusion matrix \n",
    "df_cm = pd.DataFrame(cm, index = matrix_index, columns = matrix_index)\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "sn.heatmap(df_cm, annot=annot, fmt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
